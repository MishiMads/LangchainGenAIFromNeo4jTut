{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Here are some different judge prompts for backup.",
   "id": "db568302e21701f7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    pedagogical_metric = GEval(\n",
    "        name=\"Pedagogical Quality\",\n",
    "        criteria=\"\"\"Evaluate how well the response teaches 1st-3rd grade Danish students math concepts.\n",
    "\n",
    "        High-quality pedagogical responses should:\n",
    "        - Use questions to stimulate the student's own thinking rather than giving direct answers\n",
    "        - Reference appropriate teaching strategies from the curriculum (e.g., 10'er venner, dobbelt-op, tier-venner)\n",
    "        - Use age-appropriate language that 7-9 year olds can understand\n",
    "        - Avoid directly stating the numeric answer\n",
    "\n",
    "        Consider the overall teaching effectiveness, not just checklist compliance.\"\"\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "        threshold=0.7,\n",
    "        model=local_judge,\n",
    "        strict_mode=False\n",
    "    )"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pedagogical_metric = GEval(\n",
    "    name=\"Pedagogical Quality\",\n",
    "    criteria=\"\"\"Evaluate how well the response teaches 1st-3rd grade Danish students math concepts using a holistic scoring approach.\n",
    "\n",
    "    SCORING GUIDELINES:\n",
    "    0.9-1.0: Exceptional - Uses guiding questions, references specific curriculum strategies (10'er venner, dobbelt-op, tier-venner), age-appropriate language, and avoids direct answers\n",
    "    0.7-0.89: Good - Uses at least 2-3 of the above techniques effectively with minor gaps\n",
    "    0.5-0.69: Adequate - Uses some pedagogical techniques but may give partial direct answers or lack curriculum references\n",
    "    0.3-0.49: Weak - Provides mostly direct answers with minimal pedagogical guidance\n",
    "    0.0-0.29: Poor - Gives direct numeric answers with no teaching strategy\n",
    "\n",
    "    PRIORITIES (in order):\n",
    "    1. Uses questions to stimulate thinking (most important)\n",
    "    2. Age-appropriate language for 7-9 year olds\n",
    "    3. References curriculum strategies when relevant\n",
    "    4. Avoids stating final numeric answers\n",
    "\n",
    "    Consider teaching effectiveness holistically - a response can score well even if it doesn't check every box.\"\"\",\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    threshold=0.5,  # Lower threshold to 0.5 for more flexibility\n",
    "    model=local_judge,\n",
    "    strict_mode=False\n",
    ")"
   ],
   "id": "ef3c190bd93377e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "pedagogical_metric = GEval(\n",
    "    name=\"Pedagogical Quality\",\n",
    "    riteria=\"\"\"Score this response for teaching 1st-3rd grade Danish math students.\n",
    "\n",
    "    Give 1 point for EACH item below that is TRUE:\n",
    "    1. Contains at least one question ending with \"?\"\n",
    "    2. Does NOT state the final numeric answer directly\n",
    "    3. Shows working steps or uses a teaching strategy\n",
    "\n",
    "    Total score = points earned / 3\n",
    "\n",
    "    Examples:\n",
    "    - \"Hvis 40+40=80, hvad tror du 46+38 bliver? Kan du fortsætte?\" = 3/3 = 1.0 ✅\n",
    "    - \"Prøv at dele det op. Hvad får du?\" = 2/3 = 0.67 ✅\n",
    "    - \"Svaret er 84\" = 0/3 = 0.0 ❌\n",
    "\n",
    "    Note: Rhetorical questions like \"...altså...?\" count as guiding questions.\"\"\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "        threshold=0.5,\n",
    "        model=local_judge,\n",
    "        strict_mode=False\n",
    "    )"
   ],
   "id": "c11456463fc9db7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Useful commands:",
   "id": "17c611d0d315b51b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# deepeval test run genai-integration-langchain/Scripts/test_rag.py\n",
    "\n",
    "# ----Save results to files for comparison----\n",
    "# pytest genai-integration-langchain/Scripts/test_rag.py -v > results_auto.txt\n",
    "#pytest genai-integration-langchain/Scripts/test_rag_manual.py -v > results_manual.txt\n"
   ],
   "id": "bbf25260fa2de370",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Some other stuff:",
   "id": "dcd5f4b8d56f813b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "    # Define all metrics you want to test\n",
    "    metrics = [\n",
    "        context_metric,           # Tests if retrieved context is relevant to the query\n",
    "        faithfulness_metric,      # Tests if answer is faithful to the context (uses knowledge graph)\n",
    "        answer_relevancy_metric,  # Tests if answer is relevant to the question\n",
    "        pedagogical_metric,       # Tests pedagogical quality\n",
    "    ]"
   ],
   "id": "3d2918a840efaf75"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
